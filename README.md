# NanoTorch ğŸ”¥

ä¸€ä¸ªç”¨çº¯NumPyå®ç°çš„è½»é‡çº§æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œæ¨¡ä»¿PyTorchçš„APIè®¾è®¡ï¼Œè®©ä½ æ·±å…¥ç†è§£æ·±åº¦å­¦ä¹ çš„åº•å±‚åŸç†ã€‚

## âœ¨ ç‰¹æ€§

- **ğŸ”¢ Tensorç±»**: æ”¯æŒè‡ªåŠ¨æ±‚å¯¼çš„å¼ é‡æ•°æ®ç»“æ„
- **ğŸ§  ç¥ç»ç½‘ç»œæ¨¡å—**: Linearã€ReLUã€BatchNorm2Dç­‰å¸¸ç”¨å±‚
- **âš¡ ä¼˜åŒ–å™¨**: SGDä¼˜åŒ–å™¨ï¼Œæ”¯æŒå‚æ•°æ›´æ–°
- **ğŸ“‰ æŸå¤±å‡½æ•°**: CrossEntropyLossã€MSELossç­‰æŸå¤±å‡½æ•°
- **ğŸ“Š æ•°æ®å¤„ç†**: Datasetå’ŒDataLoaderï¼Œæ–¹ä¾¿æ‰¹é‡è®­ç»ƒ
- **ğŸ› ï¸ å·¥å…·å‡½æ•°**: arangeç­‰å¼ é‡æ“ä½œå‡½æ•°

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å®‰è£…

```bash
git clone https://github.com/huanyi/nanotorch.git
cd nanotorch
pip install -e .
```

### åŸºæœ¬ä½¿ç”¨

```python
import nanotorch as nt
from nanotorch.nn import Module, Linear, ReLU
from nanotorch.optimizer import SGD
from nanotorch.loss import CrossEntropyLoss

# å®šä¹‰æ¨¡å‹
class SimpleNet(Module):
    def __init__(self):
        super().__init__()
        self.fc1 = Linear(784, 128)
        self.relu = ReLU()
        self.fc2 = Linear(128, 10)
    
    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x

# åˆ›å»ºæ¨¡å‹
model = SimpleNet()
optimizer = SGD(model.parameters(), lr=0.01)
criterion = CrossEntropyLoss()

# å‰å‘ä¼ æ’­
x = nt.randn(32, 784)  # batch_size=32, input_dim=784
y = nt.randint(0, 10, (32,))  # 10ä¸ªç±»åˆ«

# è®­ç»ƒæ­¥éª¤
logits = model(x)
loss = criterion(logits, y)

# åå‘ä¼ æ’­
optimizer.zero_grad()
loss.backward()
optimizer.step()

print(f"Loss: {loss.data:.4f}")
```

## ğŸ“ é¡¹ç›®ç»“æ„

```
nanotorch/
â”œâ”€â”€ nanotorch/          # æ ¸å¿ƒåº“ä»£ç 
â”‚   â”œâ”€â”€ __init__.py     # åŒ…åˆå§‹åŒ–
â”‚   â”œâ”€â”€ tensor.py       # Tensorç±»ï¼Œæ”¯æŒè‡ªåŠ¨æ±‚å¯¼
â”‚   â”œâ”€â”€ nn.py          # ç¥ç»ç½‘ç»œæ¨¡å—
â”‚   â”œâ”€â”€ optimizer.py    # ä¼˜åŒ–å™¨
â”‚   â”œâ”€â”€ loss.py        # æŸå¤±å‡½æ•°
â”‚   â”œâ”€â”€ dataset.py     # æ•°æ®å¤„ç†
â”‚   â””â”€â”€ utils.py       # å·¥å…·å‡½æ•°
â”œâ”€â”€ examples/           # ç¤ºä¾‹ä»£ç 
â”‚   â””â”€â”€ train_mnist.py # MNISTè®­ç»ƒç¤ºä¾‹
â”œâ”€â”€ test/              # å•å…ƒæµ‹è¯•
â”œâ”€â”€ pyproject.toml     # é¡¹ç›®é…ç½®
â””â”€â”€ README.md         # é¡¹ç›®æ–‡æ¡£
```

## ğŸ¯ æ ¸å¿ƒç»„ä»¶

### Tensor

æ”¯æŒè‡ªåŠ¨æ±‚å¯¼çš„å¼ é‡ç±»ï¼š

```python
import nanotorch as nt

# åˆ›å»ºå¼ é‡
x = nt.tensor([1.0, 2.0, 3.0], requires_grad=True)
y = nt.tensor([4.0, 5.0, 6.0], requires_grad=True)

# æ•°å­¦è¿ç®—
z = x * y + 2
z.backward()  # è‡ªåŠ¨æ±‚å¯¼

print(x.grad)  # [4.0, 5.0, 6.0]
print(y.grad)  # [1.0, 2.0, 3.0]
```

### ç¥ç»ç½‘ç»œæ¨¡å—

æä¾›å¸¸ç”¨çš„ç¥ç»ç½‘ç»œå±‚ï¼š

```python
from nanotorch.nn import Module, Linear, ReLU, BatchNorm2D

class MLP(Module):
    def __init__(self):
        super().__init__()
        self.fc1 = Linear(784, 256)
        self.bn1 = BatchNorm1D(256)
        self.relu1 = ReLU()
        self.fc2 = Linear(256, 10)
    
    def forward(self, x):
        x = self.fc1(x)
        x = self.bn1(x)
        x = self.relu1(x)
        x = self.fc2(x)
        return x
```

### ä¼˜åŒ–å™¨

SGDä¼˜åŒ–å™¨å®ç°ï¼š

```python
from nanotorch.optimizer import SGD

model = MyModel()
optimizer = SGD(model.parameters(), lr=0.01)

# è®­ç»ƒå¾ªç¯
optimizer.zero_grad()
loss = criterion(model(x), y)
loss.backward()
optimizer.step()
```

## ğŸ“ˆ ç¤ºä¾‹ï¼šMNISTæ‰‹å†™æ•°å­—è¯†åˆ«

è¿è¡ŒMNISTè®­ç»ƒç¤ºä¾‹ï¼š

```bash
cd examples
python train_mnist.py
```

**æ€§èƒ½è¡¨ç°ï¼š**
- æµ‹è¯•å‡†ç¡®ç‡ï¼š**97.60%**

## ğŸ”¬ æ¶ˆèå®éªŒ

æˆ‘ä»¬è¿›è¡Œäº†æ¨¡å‹æ¶æ„å’Œä¼˜åŒ–å™¨çš„æ¶ˆèå®éªŒï¼Œç»“æœå¦‚ä¸‹ï¼š

### å®éªŒé…ç½®
- **æ•°æ®é›†**: MNIST (784ç‰¹å¾, 10ç±»åˆ«)
- **è®­ç»ƒè½®æ•°**: 5
- **æ‰¹æ¬¡å¤§å°**: 64
- **æµ‹è¯•é›†æ¯”ä¾‹**: 10%
- **éšæœºç§å­**: 42

### æ¨¡å‹æ¶æ„å¯¹æ¯”

| æ¨¡å‹æ¶æ„ | SGD | Adam |
|---------|-----|------|
| Linear | 91.69% | 92.00% |
| Linear+ReLU | 96.50% | 97.50% |
| Linear+BN | 91.13% | 91.66% |
| Linear+ReLU+BN | 97.07% | 97.20% |

### å…³é”®å‘ç°

1. **ReLUæ¿€æ´»å‡½æ•°æ˜¾è‘—æå‡æ€§èƒ½** (+5%å‡†ç¡®ç‡æå‡)
2. **BatchNormå•ç‹¬ä½¿ç”¨æ•ˆæœä¸ä½³**ï¼Œä½†ä¸ReLUç»„åˆæ•ˆæœæœ€ä½³
3. **Adamä¼˜åŒ–å™¨åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ç•¥ä¼˜äºSGD**
4. **æœ€ä½³ç»„åˆ**: Linear+ReLU+Adam (97.50%å‡†ç¡®ç‡)
5. **æœ€å·®ç»„åˆ**: Linear+BN+SGD (91.13%å‡†ç¡®ç‡)

### æ€§èƒ½æ’å

1. Linear+ReLU+Adam: **97.50%**
2. Linear+ReLU+BN+Adam: **97.20%**
3. Linear+ReLU+BN+SGD: **97.07%**
4. Linear+ReLU+SGD: **96.50%**
5. Linear+Adam: **92.00%**
6. Linear+SGD: **91.69%**
7. Linear+BN+Adam: **91.66%**
8. Linear+BN+SGD: **91.13%**

### è¿è¡Œæ¶ˆèå®éªŒ

```bash
# è¿è¡Œæ‰€æœ‰æ¶ˆèå®éªŒ
cd examples
python train_mnist_ablation.py --ablation

# è¿è¡Œç‰¹å®šå®éªŒ
python train_mnist_ablation.py --model linear_relu --optimizer adam

# æŸ¥çœ‹æ‰€æœ‰å¯ç”¨é€‰é¡¹
python train_mnist_ablation.py --help
```

è¯¦ç»†å®éªŒæ—¥å¿—è¯·æŸ¥çœ‹: `logs/mnist_ablation_results.log`

## ğŸ§ª è¿è¡Œæµ‹è¯•

```bash
# è¿è¡Œæ‰€æœ‰æµ‹è¯•
python test/run_tests.py

# è¿è¡Œç‰¹å®šæµ‹è¯•
python test/test_tensor.py
python test/test_nn.py
```

## ğŸ“ å­¦ä¹ ä»·å€¼

NanoTorchæ˜¯ä¸€ä¸ªæ•™è‚²æ€§é¡¹ç›®ï¼Œæ—¨åœ¨å¸®åŠ©ç†è§£æ·±åº¦å­¦ä¹ æ¡†æ¶çš„æ ¸å¿ƒåŸç†ï¼š

1. **è‡ªåŠ¨æ±‚å¯¼æœºåˆ¶**ï¼šç†è§£æ¢¯åº¦å¦‚ä½•åœ¨è®¡ç®—å›¾ä¸­åå‘ä¼ æ’­
2. **å¼ é‡æ“ä½œ**ï¼šå­¦ä¹ åŸºç¡€æ•°å­¦è¿ç®—çš„å®ç°
3. **ç¥ç»ç½‘ç»œå±‚**ï¼šæŒæ¡å„ç§å±‚çš„forwardå’Œbackwardå®ç°
4. **ä¼˜åŒ–å™¨åŸç†**ï¼šäº†è§£å‚æ•°æ›´æ–°ç®—æ³•
5. **æ¨¡å—åŒ–è®¾è®¡**ï¼šå­¦ä¹ å¦‚ä½•æ„å»ºå¯æ‰©å±•çš„æ¡†æ¶

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

| ç‰¹æ€§ | NanoTorch | PyTorch |
|------|-----------|---------|
| è‡ªåŠ¨æ±‚å¯¼ | âœ… | âœ… |
| ç¥ç»ç½‘ç»œå±‚ | âœ… | âœ… |
| ä¼˜åŒ–å™¨ | âœ… | âœ… |
| GPUæ”¯æŒ | âŒ | âœ… |
| æ€§èƒ½ | åŸºç¡€ | é«˜æ€§èƒ½ |
| å­¦ä¹ éš¾åº¦ | ç®€å• | ä¸­ç­‰ |

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤Issueå’ŒPull Requestï¼

## ğŸ“„ è®¸å¯è¯

MIT License

---

â­ å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹ä½ æœ‰å¸®åŠ©ï¼Œè¯·ç»™ä¸ªStarï¼
